import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import pandas as pd
import os
import re
from collections import Counter
import matplotlib.pyplot as plt

class Vocabulary:
    def __init__(self):
        self.stoi = {"<PAD>": 0, "<UNK>": 1}
        self.itos = {0: "<PAD>", 1: "<UNK>"} # ìˆ«ìë¥¼ ë‹¨ì–´ë¡œ ë°”ê¾¸ê¸° ìœ„í•´ ì¶”ê°€ (Decodingìš©)
        self.vocab_size = 2

    def build(self, texts, max_vocab=10000):
        print("ğŸ”¨ í•™ìŠµ ë°ì´í„°ë¡œ ë‹¨ì–´ì¥ ì¬êµ¬ì¶• ì¤‘...")
        counter = Counter()
        for text in texts:
            clean_text = re.sub(r'[^a-zA-Z0-9\s]', '', str(text).lower())
            counter.update(clean_text.split())
        
        for word, _ in counter.most_common(max_vocab - 2):
            self.stoi[word] = self.vocab_size
            self.itos[self.vocab_size] = word # Reverse mapping ì €ì¥
            self.vocab_size += 1

    def get(self, word, default):
        return self.stoi.get(word, default)
    
    def __getitem__(self, word):
        return self.stoi.get(word, self.stoi["<UNK>"])

class TextTransform:
    def __init__(self, vocab, max_len=100):
        self.vocab = vocab
        self.max_len = max_len

    def __call__(self, text):
        text = str(text).lower()
        text = re.sub(r'<br\s*/?>', ' ', text)
        text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
        tokens = text.split()
        
        # <UNK> ì²˜ë¦¬ ìˆ˜ì •
        indices = [self.vocab.get(t, self.vocab["<UNK>"]) for t in tokens]
        
        if len(indices) > self.max_len:
            indices = indices[:self.max_len]
        else:
            indices += [self.vocab["<PAD>"]] * (self.max_len - len(indices))
            
        return torch.tensor(indices, dtype=torch.long)

class IMDBDataset(Dataset):
    def __init__(self, csv_path, transform=None):
        if os.path.exists(csv_path):
            self.df = pd.read_csv(csv_path)
            self.texts = self.df['review'].values
            self.labels = [1 if s == 'positive' else 0 for s in self.df['sentiment'].values]
        else:
            print("âš ï¸ íŒŒì¼ì´ ì—†ì–´ ë”ë¯¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            self.texts = ["good movie", "bad movie"] * 5
            self.labels = [1, 0] * 5
            
        self.transform = transform

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        if self.transform:
            text = self.transform(text)
        return text, torch.tensor(label, dtype=torch.float)

class SentimentRNN(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers):
        super(SentimentRNN, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        # batch_first=Trueê°€ í•™ìŠµ ì½”ë“œì— ìˆì—ˆìœ¼ë¯€ë¡œ ìœ ì§€
        self.rnn = nn.RNN(embed_dim, hidden_size, num_layers, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_size*2, 1)

    def forward(self, x):
        x = self.embedding(x)
        output, _ = self.rnn(x)
        out, _ = torch.max(output, dim=1)
        out = self.fc(out)
        return out


if __name__ == '__main__':
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}")

    # â˜… í•™ìŠµ ë•Œì™€ ë˜‘ê°™ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
    MAX_LEN = 200    
    EMBED_DIM = 64
    HIDDEN_SIZE = 128
    NUM_LAYERS = 2
    
    
    train_csv_path = r"C:\data\movie review\train.csv"
    test_csv_path = r"C:\data\movie review\test.csv" # í…ŒìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ (ì—†ìœ¼ë©´ trainìœ¼ë¡œ í…ŒìŠ¤íŠ¸)
    model_path = './model_rnn/best_rnn_model.pth'

    if not os.path.exists(test_csv_path):
        print(f"âš ï¸ í…ŒìŠ¤íŠ¸ íŒŒì¼({test_csv_path})ì´ ì—†ì–´ í•™ìŠµ íŒŒì¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        test_csv_path = train_csv_path

    # 1. ë‹¨ì–´ì¥(Vocabulary) ë³µêµ¬
    # í•™ìŠµ ë•Œ ì‚¬ìš©í•œ ë‹¨ì–´ ì‚¬ì „ì„ ê·¸ëŒ€ë¡œ ë³µêµ¬í•´ì•¼ ëª¨ë¸ì´ ë‹¨ì–´ë¥¼ ì•Œì•„ë“£ìŠµë‹ˆë‹¤.
    temp_dataset = IMDBDataset(train_csv_path) 
    vocab = Vocabulary()
    vocab.build(temp_dataset.texts) # í•™ìŠµ ë°ì´í„°ë¡œ ë¹Œë“œ!
    
    transform = TextTransform(vocab, max_len=MAX_LEN)

    # 2. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ
    print("ğŸ“ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    test_dataset = IMDBDataset(test_csv_path, transform=transform)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0) # Windowsì—ì„  workers 0 ê¶Œì¥ (ì—ëŸ¬ ì‹œ)

    # 3. ëª¨ë¸ ë¡œë“œ
    print("ğŸ§  ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘...")
    model = SentimentRNN(vocab.vocab_size, EMBED_DIM, HIDDEN_SIZE, NUM_LAYERS).to(device)
    
    if os.path.exists(model_path):
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.eval()
        print("âœ… í•™ìŠµëœ RNN ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    else:
        print(f"âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        exit()

    # 4. ì „ì²´ ì •í™•ë„ í‰ê°€
    print("ğŸš€ ì •í™•ë„ ì¸¡ì • ì‹œì‘...")
    correct = 0
    total = 0
    
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            
            # Sigmoidë¥¼ í†µê³¼ì‹œì¼œ í™•ë¥ ë¡œ ë³€í™˜ (0~1)
            probs = torch.sigmoid(outputs.view(-1))
            predicted = (probs >= 0.5).float()
            
            total += labels.size(0)
            correct += (predicted == labels.view(-1)).sum().item()

    accuracy = 100 * correct / total
    print(f"\nğŸ† ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy:.2f}%")

    # ==========================================
    # [3] ê²°ê³¼ ëˆˆìœ¼ë¡œ í™•ì¸ (ì‹œê°í™” ëŒ€ì²´)
    # ==========================================
    print("\nğŸ‘€ ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ í™•ì¸ (ìƒìœ„ 5ê°œ)")
    print("=" * 80)
    
    dataiter = iter(test_loader)
    inputs, labels = next(dataiter)
    inputs = inputs.to(device)
    
    # ì˜ˆì¸¡ ìˆ˜í–‰
    outputs = model(inputs)
    probs = torch.sigmoid(outputs.view(-1))
    predicted = (probs >= 0.5).float()

    # í…ìŠ¤íŠ¸ ë³µì› í•¨ìˆ˜ (Index -> Word)
    def decode_text(indices, vocab):
        tokens = []
        for idx in indices:
            idx = idx.item()
            if idx == vocab.stoi["<PAD>"]: continue # íŒ¨ë”©ì€ ë¬´ì‹œ
            tokens.append(vocab.itos.get(idx, "<UNK>"))
        return " ".join(tokens)

    # 5ê°œë§Œ ì¶œë ¥
    for i in range(5):
        raw_text = decode_text(inputs[i], vocab)
        pred_label = "Positive" if predicted[i].item() == 1 else "Negative"
        act_label = "Positive" if labels[i].item() == 1 else "Negative"
        prob_val = probs[i].item() * 100
        
        # ë§ìœ¼ë©´ íŒŒë€ìƒ‰(í˜¹ì€ O), í‹€ë¦¬ë©´ ë¹¨ê°„ìƒ‰(í˜¹ì€ X) í‘œì‹œ (í„°ë¯¸ë„ í™˜ê²½ì— ë”°ë¼ ìƒ‰ìƒ ì§€ì› ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
        result_mark = "âœ…" if predicted[i] == labels[i] else "âŒ"
        
        print(f"Sample {i+1} {result_mark}")
        print(f"ğŸ“ Text : {raw_text[:100]}...") # ë„ˆë¬´ ê¸°ë‹ˆê¹Œ 100ìë§Œ
        print(f"ğŸ“Š Pred : {pred_label} ({prob_val:.1f}%)")
        print(f"ğŸ·ï¸ Real : {act_label}")
        print("-" * 80)